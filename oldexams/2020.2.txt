3.
3.1 Schedules
3.1.1
T1: R(X) W(X) R(Y) W(Y) C
T2: 					R(Y) W(Y) W(Z) C
T3: 								R(X) R(Z) W(Z) C

3.1.2
T1: R(X) W(X) R(Y) W(Y) C
T2: 					R(Y) W(Y) W(Z) C
T3: 								R(X) R(Z) W(Z) C
Conflict serializable means conflict equivalent to some serial schedule.
Therefore a serial schedule is also conflict serializable.

3.1.3
T1: R(X) W(X) R(Y)			W(Y) C
T2: 				R(Y) W(Y) 			W(Z) C
T3: 										R(X) R(Z) W(Z) C
We hvae the conflicts: T1->T2, T2->T1 which forms a cycle.

3.1.4
T1: R(X)  			W(X) R(Y)			W(Y) C
T2: 		R(Y) W(Y) 	 		W(Z) C
T3: 										R(X) R(Z) W(Z) C
Cannot be generated by SPCL since T1 tries to access a shared lock for
its R(Y) action while T2 holds the exclusive lock on Y due to its W(Y) action.
Conflicts are: T2->T1, T2 -> T3, . This is conflict equivalent to
running T2, T1, T3, so it is conflict serializable.

3.1.5
T1: R(X) 				W(X) R(Y)W(Y) C
T2: 								R(Y) W(Y) W(Z) C
T3: 		R(X) R(Z) W(Z) C
This cannot be generated by C2PL, since T3 has a shared lock X for its R(X) action.
T1 will however get an exclusive lock on X at the beginning of the transaction,
making this not possible.
It can be generated by S2PL, since T1 and T3 can have the shared lock on X, and
when T3 commits, the exclusive lock can be taken by T1.
Conflict equivalent to the serial schedule: T3, T2, T1, so it is conflict serializable.
Conflicts: T3 -> T1, T3 -> T2, T1->T2

7.
7.1 Selection
7.1.1
In case of no index, we can do a linear scan through the table.
The I/O cost will be 25,000 since we have to consider each table

7.1.2
We can in worst case do an I/O pr. record in the index, since it is non-clustered.
Therefore we can risk 50,000 I/O actions.
Here it can be better to just do sequential scan.

7.2 Join and Aggregate
7.2.1
To do the join in 2 passes we have to use grace hash join
	sqrt(1,500) < 150
The I/O cost will then be 3*(25,000+1,500)=78,500 I/O

7.2.2
If it is desirable to have the output sorted, then in order to use
sort merg join we need
	sqrt(25,000+1,500)+1 = 164 buff pages
This algorithm uses the same I/O so it is just as fast

We can also do BNLJ, if we hav enough memory to only traverse the smallest block.
Let see the ratio of BNLJ and grace hash joing
	(25,000*1,500)/3*(2500+1500)
What if we could load all of 1,500 pages into memory?


7.2.3
Since we have sqrt100,000) < 500 we can do external sorting in 2 passes.
Sorting will be on productID an duplicates are handled by city attribute.
This is desirable to have sorted output on productID.

We can perform the aggregation doing the final merge of the sort.
This makes the total I/O cost of just 3 * 100,000 I/O operations

7.2.4
First we do a range-partitioning onto the 5 servers where we range on
orderID. This ensures records from the two taples with matching
orderID are on the same cluster. Then we can join in parallel on each
server and reduce the results by an ordered traversal of the servers.

First we have to redistribute the data which takes 1-1/N = 4*N/5