1 Atomocity
1.1
Before or after atomicity - actions appear to run completely before one or another or after such that no inconsistent state is present.
Concurrent transactions makes this hard due to handling of anomalies such as dirty reads. Using locks we can achieve this form of atomicity. This is also known as isolation

1.2
All or nothing atomicity
Either all finnish the operations or none does.
Primary site replication, where the subscripers either all see the update or none of them do. Using two-phase commit makes this posible.

Could also be failure in transactions where we ensure failed transactions do not execute. This is especially difficult with no-force and stealing pages
Approach: recovery protocol

1.3
Atomcity as a strongly modular mechanism.
This type of atomicity is used to hide that one larger action is made of many smallactions.
Difficult: Leaky abstractions where system expose too much of their internals
Approach: modularity through interfaces

2
2.1
Both
Since we have a centralized approach, both scenarios are bottlenecked by the speed
to which the scheduler can distribute work to the individual nodes.
First scenario
Since A has to combine the results of each works, A will itself do some of the work that the workers perform. If there are many workers, then this can bottleneck as well
Second scenario
The scheduler will have even more work to do in this case, since it has to make sure each workers has the specific information it needs to do the task
The slowest worker can be a bottleneck since the aggregator might wait for it.
Copying data can also bottleneck
Fix
We could have a hierachial approach with multiple aggregators, such that we do not overload the aggregator with work all at once, but we in parallel reduce the work.
We can also exploit data + task parallelism, so each work works on the same data (scenario 1) and each work does something different on possibly different data (scenario 2). This can ensure that en general each worker is full.

2.2
Performance evaluation

Metrics
Since the clients have an illusion of calling a service without knowing the architechture, a good metric is latency in both cases. This tells us how fast the clients perceive the service.

How to measure
We can measure latency by calling the service under different workloads and then take the average latency. We can use timers at  the scheduler and aggregator to calculate latency
Also use throughput - test the events we can handle for fixed latency

We have to measure the total time and the time of each component to know what is a bottleneck

Experiments
Yes, since scenario two assumes the workers do different data, so the experiments has to include cases where workers have different ammounts to do ans different levels of similarity between tasks and the data they process.
Scenario 1 would have to test different input sizes, since small inputs might not be worth parallelizing.

3.
3.1
3.1.1
T1: R(X) W(X) R(Y) W(Y) C T2: R(Y) W(Y) W(Z) C T3: R(X) R(Z) W(Z) C

T1:  R(X)  W(X)  R(Y) W(Y) C
T2                             R(Y)      W(Y) W(Z) C
T3                                  R(X)             R(Z) W(Z) C
This is equivalent to running T1,T2,T3 so it is equivalent to some serial schedule
Could also use serial schedule
3.1.2
T1:  R(X)  W(X)       R(Y) W(Y) C
T2                                     R(Y)        W(Y) W(Z) C
T3              R(X)              R(Z)      W(Z) C
Precedence graph: T1->T2, T3->T2 so it is acycle and conflict serializable
Could also use serial schedule

3.1.3
T1:  R(X)  W(X)       R(Y) W(Y) C
T2                                R(Y)       W(Y) W(Z) C
T3              R(X)                   R(Z)              W(Z) C
Precedence graph: T1->T2, T3->T2, T2->T3
so there is a cycle and it is therefore not conflict serializable

3.1.4
T1:  R(X)  W(X)       R(Y) W(Y) C
T2                                     R(Y)        W(Y) W(Z) C
T3              R(X)              R(Z)      W(Z) C
Since all locks are dropped at last, and T1 has an exclusive lock on X while T3 has shared lock to do its Read, then this cannot be generated by S2PL. There is no cycle so it is conflict serializable.
Precedence graph: T1->T2, T3->T2 so it is conflict serilizable

3.1.5
T1:  R(X)       W(X)  R(Y) W(Y) C
T2                                 R(Y) W(Y) W(Z) C
T3        R(X)                                      R(Z) W(Z) C
Precedence: T1->T3, T1->T2, T2->T3
Can not be generated by C2PL since, T3 reads X, while T1 has exclusive lock on X to perform its lock. This is allowed for S2PL since T1 would only get exclusive lock just before performing W(X)

3.2
3.2.1
S2PL can generate deadlocks
T1:  W(Y)      R(X)
T2:       W(X)      R(Y)
T1 holds an exclusive lock on Y and T2 am exc√¶isove lock on X. T1 needs shared lock for the R(X) action, but it cannot be granted before T1 drops its exclusive lock on X. However, T2 holds the lock until it has done R(Y). But, T2 can only do R(Y) when it receives the shared lock on Y (T1 would drop the exclusive lock). But T1 already waited on T2.

Conservative 2PL will not deadlock because all locks are acquired upfront, and we wait until the locks are available otherwise. This would mean that T1 and T2 get all the locks they need instead upfrong - this is instead of either of them waiting for other transactions to drop locks while they already have some locks (which is what caused deadlocks).


3.2.2
The first case is wound-wait deadlock prevention. Since a higher priority transaction Ti
can never wait for a lower priority transaction, no cycles will develop.

The other case is a type of wait-die deadlock prevention that assigns priority by number of
locks. It does however give rise to deadlocks
T1:  W(Y)  R(A)     R(X)
T2:             W(X)      R(A) R(Z) R(Y)
When T1 requests the shared lock on X, it must wait for T2 to drop its exclusive lock since T1 has 2 locks and T2 has 1 lock. Then when T2 requests shared lock on Y, it has trhee locks and T1 has two locks(T1 has not gotten S(X) yet), so T2 waits. Now they both wait for each other.

T1: X(A)                       X(D) X(E) X(B) Wait
T2       X(B) X(C) X(A)[waits]

4.
4.1
LSN    PREV_LSN     XACT_ID    TYPE    PAGE_ID    UNDONEXTLSN
1   						   B_CKPT
2  							   E_CKPT
3.       -            T3       Update  P42
4.       3            T3       Commit  -
5.       -            T2       Update  P154
6.       -            T1       Update  P42
7.       5            T2       Abort   -
8.       6            T1       update  P99
### Crash####

4.2
1. Losers = {T2, T1} aborted or running, Winners = {T3} commited
2. redo-start:3 (min recLSN in dirty page table), undo-end: 5(oldest LSN of loser)
3. {3,5,6,8} (all update log records, and memory might not have been flushed)
4. {8, 7, 6, 5} (undo all updates of losers)
5.
LSN    PREV_LSN     XACT_ID    TYPE    PAGE_ID    UNDONEXTLSN
1   						   B_CKPT
2  							   E_CKPT
3.       -            T3       Update  P42
4.       3            T3       Commit  -
5.       -            T2       Update  P154
6.       -            T1       Update  P42
7.       5            T2       Abort   -
8.       6            T1       update  P99
### Crash####
9.       4            T3       Commit               -
10.      8.           T1       Abort                -
11.      10           T1       CLR      p99         6
12.      11           T1       CLR                  -
13.      12           T1       END                  -
14       5            T2       CLR                  -
15       14           T2       END

5.
5.1
5.1.1
P1: e1(1)e3(2) e5(3)  e7(4)  e9(5)  e10(6)
P2:      e2(2)  e6(3)
P3:            e4(3)        e8(4)  e11(5)

5.1.2
P1: e1(1,0,0), e3(2,0,0), e5(3,1,0), e7(4,1,0), e9(5,2,0), e10(6,2,1)
P2: e2(1,1,0), e6(2,2,0)
P3: e4(2,0,1), e8(2,0,2), e11(4,1,3)

5.1.3
Lamport: No. This is because timestamp e4 < timestamp e5 this does not imply that e4 happened before e5.
Vector clocks: No because we have e4 = (2,0,1) and e5=(3,1,0) which means that
e4 ||, so they are in conflict.

5.1.4
Shipment can use vector clocks to see that e8 preceedes e4, and therefore it should cancel the order and not reply. When it receive e11 it should however reply with an ack.

5.2
5.2.1
Synchronous:
The replicas will always be in sync so we can never read something is still
in stock when it is sold out.
The writes are however slow, and we must use 2PC
Asynchrnous:
Faster writes since we do not wait for response, but the replicas can come out
of sync.

5.2.2
We should have that total number of quoroms is
	Qr + Qw > 5 (number of replicas).
Since we do not have this, then we might have inconsistency in the reads.
This happens because we might have a site without the newest version.

5.3.3
b) Asynchronous peer to peer. We increase throughput, since we have small
master records (or parts of a recrod), and updates to the service can be done
in parallel at each site on its part of the master record. We must ensure the updates are propagated and conflicts are resolved. We increase throughput since any peer can get the update. More scalable than primary site, since we can have multiple copies by default.

6.
No it does not. We might have a situation where the payments send back an acknoledgement,
and the shipment crashes and order crashses. In this case the orders and shipments are out of sync,
and since the order also crashed no one can make them come in sync again.

Even worse only the shipment can crash, while the others work and this will just block